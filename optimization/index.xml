<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Numerical optimization on SC1</title>
    <link>sc1/optimization/</link>
    <description>Recent content in Numerical optimization on SC1</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="sc1/optimization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Numerical Optimisation</title>
      <link>sc1/optimization/numerical-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>sc1/optimization/numerical-optimization/</guid>
      <description>An optimisation problem is the problem of finding the best solution from all feasible solutions. There are genenally speaking two types of optimisation problems: continuous or discrete, which refers to whether the variables are continuous or discrete. We will focus on continuous optimisation problems. Without any loss of generality, just as in R functions, we try to minimise (rather than maximise) a function.
The standard form of a continuous optimisation problem is as follows:</description>
    </item>
    
    <item>
      <title>R Optimisation Functions</title>
      <link>sc1/optimization/r-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>sc1/optimization/r-optimization/</guid>
      <description>A Two-dimensional Example To illustrate various R optimisation functions, we define a simple two-dimensional function below and make a contour plot. We use the meshgrid function from the package pracma, which gives \(x\) and \(y\) coordinates of a square grid in \(\mathbb{R}^2\).
library(pracma) # for meshgrid function f &amp;lt;- function(x1, x2) cos(x1-1) + cos(x2) + sin(3*x1*x2) + x1^2 + x2^2 meshgrid(seq(-2, 2, length=5)) ## $X ## [,1] [,2] [,3] [,4] [,5] ## [1,] -2 -1 0 1 2 ## [2,] -2 -1 0 1 2 ## [3,] -2 -1 0 1 2 ## [4,] -2 -1 0 1 2 ## [5,] -2 -1 0 1 2 ## ## $Y ## [,1] [,2] [,3] [,4] [,5] ## [1,] -2 -2 -2 -2 -2 ## [2,] -1 -1 -1 -1 -1 ## [3,] 0 0 0 0 0 ## [4,] 1 1 1 1 1 ## [5,] 2 2 2 2 2 x &amp;lt;- seq(-2, 2, length=101) grid_XY &amp;lt;- meshgrid(x) z &amp;lt;- matrix(mapply(f, grid_XY$X, grid_XY$Y), nrow=101) min(z) ## [1] 0.</description>
    </item>
    
    <item>
      <title>Gradient Descent and Stochastic Gradient Descent</title>
      <link>sc1/optimization/sgd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>sc1/optimization/sgd/</guid>
      <description>Gradient descent implementation We try to solve the previous nonlinear least squares problem using gradient descent. The difference in magnitudes of optimal parameter values for \(b\) in this example causes gradient descent algorithm to converge very slowly, if at all (have a try yourself). To illustrate the working of the gradient descent algorithm, we scale \(b_3\) by 100 in the code below so that the optimal values of \(b\) are of roughly the same magnitude.</description>
    </item>
    
  </channel>
</rss>
